{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<div  style=\"padding: 0px; padding-left: 0px;\">\n",
    "<img src=\"files/lightning_logo.png\" width=\"320px\"  style=\"display: inline-block; margin-left: 0px;\">\n",
    "</div>\n",
    "\n",
    "<p class='header-1'>Welcome to <span style='font-weight:800'>Lightning</span></p>\n",
    "\n",
    "<p class='info-text'>Lightning is intended to be an open-source library for integrating and analyzing spatial data obtained from a variety of imaging modalities. Lightning is capable of working with small, medium, and very large data sets. Like its sibling, <span style='font-weight:800'>Thunder</span>, it will expose array operations through either local or distributed implementations with a common interface, and makes it easy to switch between them. Its distributed implementation currently targets Spark, a powerful cluster computing framework. \n",
    "\n",
    "<p class='info-text'>To see the internals of Lightning, visit the <a href='http://github.com/alexandonian/lightning'>project</a> page. \n",
    "</p>\n",
    "\n",
    "<p class='header-2'>Tutorials</p>\n",
    "<p class='info-text'>These notebooks are interactive tutorials that show how Lightning works:</p>\n",
    "\n",
    "<a class='call-link lightning' href='tutorials/Basic-Usage.ipynb'>Basic usage (on its way)</a>\n",
    "\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class='header-2'>Introduction</span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class='info-text'>The overarching theme behind Lightning is ‘fusion’ at multiple scales: fusion of imaging data,fusion of the latest algorithms/tools, fusion of its applications. The original motivation behind Lightning was to develop and test spatial intratumor heterogeneity metrics that correlate with particular cancer subtypes using deep neural networks. Our particular goal was to establish an image analysis pipeline that would provide end-to-end, automated classification of multiplexed immunofluorescence images obtained from tissue microarray sections. However, since we are interested in more than tissue microarray data and classification tasks, our project has quickly expanded into the development of a pipeline for integrating and analyzing data along the imaging continuum. We hope to include:\n",
    "</p>\n",
    "<ul class='info-bullet'>\n",
    "<li>Light microscopy</li>\n",
    "<li>Fluorescent and immunofluorescent (IF) imaging</li>\n",
    "<li>Immunohistochemistry (IHC)</li>\n",
    "<li>Histopathology (H&E)</li>\n",
    "<li>Mass-Spec imaging (IMS)</li>\n",
    "<li>In-situ hybridization imaging (ISH)</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "<p class='info-text' >To facilitate the formation and analysis of these multimodal images, we will use a ‘fusion’ of the latest, state-of-the-art algorithms and tools. These include:\n",
    "<ul class='info-bullet'>\n",
    "<li>EM patch-based CNNs (summer 2016)</li>\n",
    "<li>Image Fusion using multivariate regression </li>\n",
    "<li>Nonlinear image registration - registration is pre-req for image fusion (fall 2016, winter 2017)</li>\n",
    "<li>Spatial statistics (summer, fall 2016)</li>\n",
    "<li>Interactive visualization - for clinical use</li>\n",
    "</ul>\n",
    "</p>\n",
    "\n",
    "<p class='info-text'>Lightning is not meant to be used strictly by the researcher to study the structure and progression of diseased tissue. The hope is for Lightning to be used as a powerful visualization tool and diagnostic advisor for the clinician.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p class='header-2'>Rethinking the Image in Lightning:</span></p>\n",
    "\n",
    "<p class='info-text'>When we think about the structure of a “traditional” grayscale image, we typically imaging a 2D array representing the spatial dimensions height and width with each element representing the intensity at that spatial location. For color images, we add a third dimension to accommodate the three color channels red, green and blue.<p/>\n",
    "\n",
    "<p class='info-text'>In contrast, most imaging modalities natively deliver their measurements as an $n$th order tensor (multidimensional array of values that can be accessed via $n$ indices). More specifically, an imaging modality that records along $n_s$ spatial dimensions and acquires an $n_m$  dimensional array at each measurement location delivers an $n = n_s + n_m$ order tensor. To ground this discussion, typical examples include:</p>\n",
    "<ul class='info-bullet'>\n",
    "<li>$n_s = 0$: a single measurement</li>\n",
    "<li>$n_s = 1$: a list of measurements</li>\n",
    "<li>$n_s = 2$: 2D imaging (e.g. light microscopy)</li>\n",
    "<li>$n_s = 3$: 3D imaging (e.g. MRI)</li>\n",
    "</ul>\n",
    "<p class='info-text'>Similarly, the number of measurement modes can also vary:\n",
    "<ul class='info-bullet'>\n",
    "<li> $n_m = 0$ scalar value measured (e.g.grayscale image) </li>\n",
    "<li> $n_m = 1$ : vector of measured values (e.g. RGB image when len(vector) = 3)</li>\n",
    "<li> $n_m = 2$ : array of measured values at locations</li>\n",
    "<li>And so on..</li>\n",
    "</ul>\n",
    "\n",
    "<p class='info-text'> In most cases, we will be dealing with 3rd order tensors, (i.e. two spatial dimensions and a 1 dimensional measurement vector).</p>\n",
    "\n",
    "\n",
    "\n",
    "<p class='header-3'>Internal representation of an image (Lightning)</p>\n",
    "<p class='info-text'>In order to accommodate data acquired from a variety of imaging modalities, we will apply a “flattening” procedure to convert data into a tabular format (2D array). Each row in this array will represent a particular spatial location and each column will represent a particular feature or measurement made across all spatial locations. (I believe this will also lend itself nicely to Spark computations).</p>\n",
    "\n",
    "\n",
    "<div style='float:left;'>\n",
    "<img src='files/Flattening.png' width='400px'style=\"display: inline-block; margin-left: 0px;\">\n",
    "</div>\n",
    "\n",
    "\n",
    "<p class='info-text' style='margin-bottom:50px'><b>Figure 1</b>. Example of the flattening procedure. An RGB image with 2 spatial dimensions and one measurement dimension with three channels (red, green and blue) is converted to a 2D array. The rows represent individual pixels and the colums correspond red, green and blue measurements at that pixel.\n",
    "</p>\n",
    "\n",
    " \n",
    "<p class='info-text'> <span style='font-weight:800'>Advantage:</span> In this way, an image can be viewed more as a detailed description of the “state” of a tissue that makes a point to use the strong suits of each imaging modality. For example, we can enhance the spatial specificity of mass-spec imaging, which has high chemical specificity, but fusing with H&E images. We can start to incorporate genetic information such as maps of gene expression into our description of the state of the tissue. </p>\n",
    "\n",
    "<p class='header-3'> Handling the Image Registration Problem (Lightning-Register)</p>\n",
    "<p class='info-text'>\n",
    "\n",
    "</p>\n",
    "\n",
    "<p class='header-3'> Handling the Multi-resolution Problem (Lightning-Fuse)</p>\n",
    "<p class='info-text'>Now that the images are aligned, the spatial domian is common to all data sources. Even still, it may be difficult to establish a one-to-one mapping between measurements of different technical origin due to the different spatial resolution scales. The naive approach would be to downsample images to the match the modality with the lowest spatial approach. A better approach would be to apply a <b>one-to-many mapping</b> whereby many observations in a higher resolution modality is mapped to a single observation in a lower resolution modality (for more info see Raf Van De Plas 2015) </p>\n",
    "\n",
    "<img src='files/Multi-Resolution.png' width='200px'style=\"display: margin-left: 0px; align:middle;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class='header-2'>Image Analysis</p>\n",
    "<p class='info-text'>(Coming soon!)\n",
    "<p class='header-3'>Spatial Statistics as Description of ITH</p>\n",
    "<p class='info-text'>(Coming soon!)\n",
    "<p class='header-3'>Automatated, end-to-end Classification of Cancer Subtypes</p>\n",
    "<p class='info-text'>(Coming soon!)\n",
    "<p class='header-3'>Computational Anatomy</p>\n",
    "<p class='info-text'>(Coming soon!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class='header-2'>Proposed Pipeline</p>\n",
    "\n",
    "<div  style=\"padding: 0px; padding-left: 0px;\">\n",
    "<img src=\"files/Proposed_Image_Pipeline.png\" width=\"800px\"  style=\"display: inline-block; margin-left: 0px;\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p class='header-3'> Provider</p>\n",
    "<p class='header-3'> ImageController</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p class='header-2'>References</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
